{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes that you have a basic command of the Python programming language. If you have no experience with Python, there are innumerable great tutorials and introductions out there. You can find a good overview of some of them [here](https://wiki.python.org/moin/BeginnersGuide/Programmers).\n",
    "\n",
    "It also assumes that you have gone through following sessions in the [Text Analysis with Python series](https://git.dartmouth.edu/lib-digital-strategies/RDS/workshops/text-analysis/text-analysis-with-python):\n",
    "- Strings and Files\n",
    "- Word counts\n",
    "- TF/IDF\n",
    "- Topics and Emotions\n",
    "\n",
    "Finally, we will use the machine learning toolkit `scikit-learn` and assume that you have gon the [Intro to Machine Learning with scikit-learn](https://git.dartmouth.edu/lib-digital-strategies/RDS/workshops/machine-learning/intro-to-machine-learning-with-scikit-learn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you are dealing with a number of different pieces of writing and want to categorize them into a set of predefined groups. For example, you might want to find out the language a particular text is written in. Or you are dealing with an email at work and are trying to figure out which department to forward it to. In text classification, you assign a *class* to each piece of text. So, for example, you assign the class `English` or `German` to a document, or the class `accounting` or `customer service` to the email.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "How would you, as a human, solve these examples? Think about it step-by-step: How would you mentally process the text, what would inform your decision?\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this manually for smaller amounts of text, but we of course quickly run into problems at a larger scale: You would not want to classify every single incoming email at Dartmouth as `spam` or `no spam`, of course. This is where algorithmic text classification using machine learning can help!\n",
    "\n",
    "Text Classification is a three-step process:\n",
    "1. Extract descriptive features from a sufficiently large number of texts belonging to known categories/classes\n",
    "2. Train a classifier using these features to discriminate between these classes\n",
    "3. Use the trained classifier to classify new text pieces\n",
    "\n",
    "In this session, we will walk through this process by building up a classifier that can tell us if any given State of the Union address was written by a Republican or a Democratic president. If you are running this notebook on Dartmouth's JupyterHub, the dataset is already available to you under `~/shared/RR-workshop-data/state-of-the-union-dataset/txt`. Otherwise you can download the dataset [here](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/state-of-the-union-dataset/-/archive/main/state-of-the-union-dataset-main.zip) and put the in a folder of your choosing.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "**Caveat emptor:** This particular system we are building here is most likely not the optimal system for this task. There are literally thousands of models and algorithms we could choose from and even more feature sets we could consider. The main purpose of this notebook, however, is to give you a relatively simple example that will hopefully give you a good idea of how text classification works *in principle*. Maybe you even feel inspired to engineer your own features or try out different classifiers?\n",
    "\n",
    "If you do, please [let us know how it went](mailto:simon.stone@dartmouth.edu?subject=Text%20classification%20workshop)!\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the examples in our training set need to be already labeled (i.e., marked as *Republican* or *Democrat*). Fortunately, the State of the Union dataset includes some meta information we can use to do that. Since this kind of processing is outside the scope of this notebook, we moved this task to a separate notebook `add-meta.ipynb`. If you are interested, you can open that notebook and see how it works excactly, but for now we will simply run that other notebook using a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run add-meta.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "**Note:** If your State of the Union dataset is not at the default location (e.g., you are not working on Dartmouth's JupyterHub), you may have to open `add-meta.ipynb` and change the variable `dataset_folder`  accordingly!\n",
    "</div>\n",
    "\n",
    "That notebook produced a CSV file we can now read using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Party</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2009</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1874</td>\n",
       "      <td>Ulysses S. Grant</td>\n",
       "      <td>Ulysses S.</td>\n",
       "      <td>Grant</td>\n",
       "      <td>Republican</td>\n",
       "      <td>To the Senate and House of Representatives:\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1984</td>\n",
       "      <td>Ronald Reagan</td>\n",
       "      <td>Ronald</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Mr. Speaker, Mr. President, distinguished Memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1995</td>\n",
       "      <td>Bill Clinton</td>\n",
       "      <td>Bill</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr. President, Mr. Speaker, members of the 104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1920</td>\n",
       "      <td>Woodrow Wilson</td>\n",
       "      <td>Woodrow</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year              Name  First Name Last Name       Party  \\\n",
       "218  2009      Barack Obama      Barack     Obama  Democratic   \n",
       "84   1874  Ulysses S. Grant  Ulysses S.     Grant  Republican   \n",
       "193  1984     Ronald Reagan      Ronald    Reagan  Republican   \n",
       "204  1995      Bill Clinton        Bill   Clinton  Democratic   \n",
       "130  1920    Woodrow Wilson     Woodrow    Wilson  Democratic   \n",
       "\n",
       "                                                  Text  \n",
       "218  Madame Speaker, Mr. Vice President, Members of...  \n",
       "84   To the Senate and House of Representatives:\\n\\...  \n",
       "193  Mr. Speaker, Mr. President, distinguished Memb...  \n",
       "204  Mr. President, Mr. Speaker, members of the 104...  \n",
       "130  GENTLEMEN OF THE CONGRESS:\\n\\nWhen I addressed...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sotu = pd.read_csv('data/sotu-extended.csv')\n",
    "# Look at five random samples from the dataframe\n",
    "sotu.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Numeric features and encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of feature extraction is to *define* and *extract* features that hopefully best help to distinguish the classes of interest. Basically anything that describes the content of the analyzed texts can be a feature, *as long as you can express it as a number*. \n",
    "\n",
    "This limitation is imposed by the fact that most machine learning algorithms work with numeric computations and have no way of dealing with more abstract concepts like \"meaning\" or \"context\".\n",
    "\n",
    "We already talked about some numeric features in previous sessions: \n",
    "- word count\n",
    "- word frequencies\n",
    "- TF/IDF\n",
    "\n",
    "But even features that are not immediately numeric can be expressed in numbers (i.e., *encoded*). Let's take for example the feature *emotion*. Intuitively, this feature would have some descriptive levels like *sad*, *happy*, or *angry*. But we could also express these as numbers, if we mark the presence of one of these emotions in a text using `1` (for present) and `0` (for absent):\n",
    "\n",
    "\n",
    "<style type=\"text/css\" >\n",
    "table {\n",
    "    border-collapse: collapse;\n",
    "    text-align: center;\n",
    "    border-top: 3px solid;\n",
    "    border-bottom: 3px solid;\n",
    "}\n",
    "\n",
    "tr, td, th {\n",
    "    border-bottom: none !important;\n",
    "    border-left: none !important;\n",
    "    border-right: none !important;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Text</th>\n",
    "    <th>Sad</th>\n",
    "    <th>Happy</th>\n",
    "    <th>Angry</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"My favorite show has been cancelled.\"</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>\"I got a promotion at work.\"</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>\"My phone battery died just when I needed it the most.\"</td>\n",
    "  <td>0</td>\n",
    "  <td>0</td>\n",
    "  <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"I was happy to see my childhood home one last time before it was sold.</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "**Note:** We can even express mixed emotions this way! Just take a look at the last example sentence.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document- versus sentence-level features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important thing to consider when you want to extract features is the *level* at which they are extracted.\n",
    "\n",
    "So far, we have only extracted features at a *document level*: Counting the words in the entire text, determmining the word frequencies in the entire text, and so on.\n",
    "\n",
    "But a *document* consists of *paragraphs*, a *paragraph* consists of *sentences*, and a *sentence* consists of *words*. You could even find more units, like clauses or even letters.\n",
    "\n",
    "So almost every unit of text is actually a sequence of smaller units. You could consider extracting features at any of these levels. For example, you could extract the word count in every sentence instead of for an entire document.\n",
    "\n",
    "The *representation* of each document in that case also becomes a sequence. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At a document-level, we observe the following features:\n",
      "word_frequencies = [('about', 4), ('this', 3), ('is', 3), ('.', 3), ('bunnies', 3), ('text', 2), ('but', 2), ('not', 2), ('i', 2), (',', 2), ('a', 1), ('cats', 1), ('and', 1), ('dogs', 1), ('cannot', 1), ('stress', 1), ('enough', 1), ('how', 1), ('little', 1), ('like', 1), ('them.', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "a_text = 'This is a text about cats and dogs . But not about bunnies . I cannot stress enough , how little this text is about bunnies . I like bunnies , but this is not about them.'\n",
    "\n",
    "# Document-level feature extraction\n",
    "word_frequencies = Counter(a_text.lower().split()).most_common()\n",
    "\n",
    "\n",
    "print(\"At a document-level, we observe the following features:\")\n",
    "print(f\"{word_frequencies = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "As you can see, document-level features lose all the structure and context inherent in a document. The basic assumption here is that the relative order of the words does not matter for the intended purposes. This perspective on a text is therefore often called the *bag-of-words* model.\n",
    "</div>\n",
    "\n",
    "So if we look at this text at a document level, we might conclude that it is about bunnies! The word bunnies appears 3 times, after all. But if we look at it at a sentence level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At a sentence level, we observe the following sequence of features:\n",
      "Sentence 0:\n",
      "[('this', 1), ('is', 1), ('a', 1), ('text', 1), ('about', 1), ('cats', 1), ('and', 1), ('dogs', 1)]\n",
      "Sentence 1:\n",
      "[('but', 1), ('not', 1), ('about', 1), ('bunnies', 1)]\n",
      "Sentence 2:\n",
      "[('i', 1), ('cannot', 1), ('stress', 1), ('enough', 1), (',', 1), ('how', 1), ('little', 1), ('this', 1), ('text', 1), ('is', 1), ('about', 1), ('bunnies', 1)]\n",
      "Sentence 3:\n",
      "[('i', 1), ('like', 1), ('bunnies', 1), (',', 1), ('but', 1), ('this', 1), ('is', 1), ('not', 1), ('about', 1), ('them', 1)]\n",
      "Sentence 4:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"At a sentence level, we observe the following sequence of features:\")\n",
    "for idx, sentence in enumerate(a_text.split('.')):\n",
    "    print(f'Sentence {idx}:')\n",
    "    print(Counter(sentence.lower().split()).most_common())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the word `bunnies` always occurs together with some form of `not` in the same sentence. So maybe this text is not about bunnies, after all?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "This (somewhat crude) example demonstrates that sentence-level features are much better at capturing *local context*. However, since sentence-level features are more complex to process (see below), there are other techniques like N-grams and collocations that try to achieve the same thing while still remaining at the document level.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this, you might be tempted to always go with sentence-level features. However, where a document-level approach converts a document into one set of features (a *feature vector*), a sentence-level approach converts the document into a *sequence* of feature vectors. The problem here is that conventional machine learning models can only process fixed-size feature vectors, not sequences of them. Neural networks, on the other hand, can be very good at handling sequences, which is why Large Language Models (like ChatGPT) are so good at what they do. \n",
    "\n",
    "Using neural networks, though, comes with many challenges regarding the complexity of the models involved and, above all, the amount of data needed to train them. For many tasks, it is therefore advisable to follow [the principle of parsimony](https://en.wikipedia.org/wiki/Occam's_razor) and use document-level features. They can still get the job done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from the State of the Union addresses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace all occurrences of these punctuation marks:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      Fellow Citizens of the Senate  and House of Re...\n",
       "1      Fellow Citizens of the Senate and House of Rep...\n",
       "2      Fellow Citizens of the Senate and House of Rep...\n",
       "3      Fellow Citizens of the Senate and House of Rep...\n",
       "4      Fellow Citizens of the Senate and House of Rep...\n",
       "                             ...                        \n",
       "223    Mr  Speaker  Mr  Vice President  Members of Co...\n",
       "224    Mr  Speaker  Mr  Vice President  Members of Co...\n",
       "225    Mr  Speaker  Mr  Vice President  Members of Co...\n",
       "226    Thank you very much  Mr  Speaker  Mr  Vice Pre...\n",
       "227    Mr  Speaker  Mr  Vice President  Members of Co...\n",
       "Name: Text, Length: 228, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print('Replace all occurrences of these punctuation marks: ', string.punctuation)\n",
    "for symbol in string.punctuation:\n",
    "    sotu['Text'] = sotu['Text'].str.replace(symbol, ' ', regex=False)\n",
    "   \n",
    "sotu['Text']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Party</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1790</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate  and House of Re...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1791</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1793</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2014</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2015</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2017</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Thank you very much  Mr  Speaker  Mr  Vice Pre...</td>\n",
       "      <td>[Thank, you, very, much, Mr, Speaker, Mr, Vice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2018</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year               Name First Name   Last Name         Party  \\\n",
       "0    1790  George Washington     George  Washington  Unaffiliated   \n",
       "1    1791  George Washington     George  Washington  Unaffiliated   \n",
       "2    1792  George Washington     George  Washington  Unaffiliated   \n",
       "3    1793  George Washington     George  Washington  Unaffiliated   \n",
       "4    1794  George Washington     George  Washington  Unaffiliated   \n",
       "..    ...                ...        ...         ...           ...   \n",
       "223  2014       Barack Obama     Barack       Obama    Democratic   \n",
       "224  2015       Barack Obama     Barack       Obama    Democratic   \n",
       "225  2016       Barack Obama     Barack       Obama    Democratic   \n",
       "226  2017       Donald Trump     Donald       Trump    Republican   \n",
       "227  2018       Donald Trump     Donald       Trump    Republican   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    Fellow Citizens of the Senate  and House of Re...   \n",
       "1    Fellow Citizens of the Senate and House of Rep...   \n",
       "2    Fellow Citizens of the Senate and House of Rep...   \n",
       "3    Fellow Citizens of the Senate and House of Rep...   \n",
       "4    Fellow Citizens of the Senate and House of Rep...   \n",
       "..                                                 ...   \n",
       "223  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "224  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "225  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "226  Thank you very much  Mr  Speaker  Mr  Vice Pre...   \n",
       "227  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "\n",
       "                                                Tokens  \n",
       "0    [Fellow, Citizens, of, the, Senate, and, House...  \n",
       "1    [Fellow, Citizens, of, the, Senate, and, House...  \n",
       "2    [Fellow, Citizens, of, the, Senate, and, House...  \n",
       "3    [Fellow, Citizens, of, the, Senate, and, House...  \n",
       "4    [Fellow, Citizens, of, the, Senate, and, House...  \n",
       "..                                                 ...  \n",
       "223  [Mr, Speaker, Mr, Vice, President, Members, of...  \n",
       "224  [Mr, Speaker, Mr, Vice, President, Members, of...  \n",
       "225  [Mr, Speaker, Mr, Vice, President, Members, of...  \n",
       "226  [Thank, you, very, much, Mr, Speaker, Mr, Vice...  \n",
       "227  [Mr, Speaker, Mr, Vice, President, Members, of...  \n",
       "\n",
       "[228 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "sotu['Tokens'] = sotu['Text'].apply(WhitespaceTokenizer().tokenize)\n",
    "sotu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Party</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens w/o stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1790</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate  and House of Re...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "      <td>[Fellow, Citizens, Senate, House, Representati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1791</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "      <td>[Fellow, Citizens, Senate, House, Representati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "      <td>[Fellow, Citizens, Senate, House, Representati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1793</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "      <td>[Fellow, Citizens, Senate, House, Representati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>Fellow Citizens of the Senate and House of Rep...</td>\n",
       "      <td>[Fellow, Citizens, of, the, Senate, and, House...</td>\n",
       "      <td>[Fellow, Citizens, Senate, House, Representati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2014</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2015</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2017</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Thank you very much  Mr  Speaker  Mr  Vice Pre...</td>\n",
       "      <td>[Thank, you, very, much, Mr, Speaker, Mr, Vice...</td>\n",
       "      <td>[Thank, much, Mr, Speaker, Mr, Vice, President...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2018</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Mr  Speaker  Mr  Vice President  Members of Co...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, of...</td>\n",
       "      <td>[Mr, Speaker, Mr, Vice, President, Members, Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year               Name First Name   Last Name         Party  \\\n",
       "0    1790  George Washington     George  Washington  Unaffiliated   \n",
       "1    1791  George Washington     George  Washington  Unaffiliated   \n",
       "2    1792  George Washington     George  Washington  Unaffiliated   \n",
       "3    1793  George Washington     George  Washington  Unaffiliated   \n",
       "4    1794  George Washington     George  Washington  Unaffiliated   \n",
       "..    ...                ...        ...         ...           ...   \n",
       "223  2014       Barack Obama     Barack       Obama    Democratic   \n",
       "224  2015       Barack Obama     Barack       Obama    Democratic   \n",
       "225  2016       Barack Obama     Barack       Obama    Democratic   \n",
       "226  2017       Donald Trump     Donald       Trump    Republican   \n",
       "227  2018       Donald Trump     Donald       Trump    Republican   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    Fellow Citizens of the Senate  and House of Re...   \n",
       "1    Fellow Citizens of the Senate and House of Rep...   \n",
       "2    Fellow Citizens of the Senate and House of Rep...   \n",
       "3    Fellow Citizens of the Senate and House of Rep...   \n",
       "4    Fellow Citizens of the Senate and House of Rep...   \n",
       "..                                                 ...   \n",
       "223  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "224  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "225  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "226  Thank you very much  Mr  Speaker  Mr  Vice Pre...   \n",
       "227  Mr  Speaker  Mr  Vice President  Members of Co...   \n",
       "\n",
       "                                                Tokens  \\\n",
       "0    [Fellow, Citizens, of, the, Senate, and, House...   \n",
       "1    [Fellow, Citizens, of, the, Senate, and, House...   \n",
       "2    [Fellow, Citizens, of, the, Senate, and, House...   \n",
       "3    [Fellow, Citizens, of, the, Senate, and, House...   \n",
       "4    [Fellow, Citizens, of, the, Senate, and, House...   \n",
       "..                                                 ...   \n",
       "223  [Mr, Speaker, Mr, Vice, President, Members, of...   \n",
       "224  [Mr, Speaker, Mr, Vice, President, Members, of...   \n",
       "225  [Mr, Speaker, Mr, Vice, President, Members, of...   \n",
       "226  [Thank, you, very, much, Mr, Speaker, Mr, Vice...   \n",
       "227  [Mr, Speaker, Mr, Vice, President, Members, of...   \n",
       "\n",
       "                                  Tokens w/o stopwords  \n",
       "0    [Fellow, Citizens, Senate, House, Representati...  \n",
       "1    [Fellow, Citizens, Senate, House, Representati...  \n",
       "2    [Fellow, Citizens, Senate, House, Representati...  \n",
       "3    [Fellow, Citizens, Senate, House, Representati...  \n",
       "4    [Fellow, Citizens, Senate, House, Representati...  \n",
       "..                                                 ...  \n",
       "223  [Mr, Speaker, Mr, Vice, President, Members, Co...  \n",
       "224  [Mr, Speaker, Mr, Vice, President, Members, Co...  \n",
       "225  [Mr, Speaker, Mr, Vice, President, Members, Co...  \n",
       "226  [Thank, much, Mr, Speaker, Mr, Vice, President...  \n",
       "227  [Mr, Speaker, Mr, Vice, President, Members, Co...  \n",
       "\n",
       "[228 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import corpus\n",
    "\n",
    "stopwords = corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if not word in stopwords]\n",
    "\n",
    "sotu['Tokens w/o stopwords'] = sotu['Tokens'].apply(remove_stopwords)\n",
    "sotu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class MySentimentIntensityAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):        \n",
    "        super().__init__()\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        out = []\n",
    "        for x in X:\n",
    "            ps = self.sia.polarity_scores(x)\n",
    "            out.append([ps['pos'], ps['neu'], ps['neg']])\n",
    "        return np.array(out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare the Pipeline \"\"\"\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='content', \n",
    "                             preprocessor=lambda x: x,   # We did the preprocessing ourselves, so just pass everything through\n",
    "                             tokenizer=lambda x: x,      # We also did the tokenization ourselves\n",
    "                             token_pattern=None,         # Since we do not tokenize, we can avoid a warning by setting this to None\n",
    "                             max_features=1000)          # Limit the vocabulary to 1000 words\n",
    "\n",
    "feature_extractor = ColumnTransformer([\n",
    "    ('tfidf', vectorizer, 'Tokens w/o stopwords'),\n",
    "    ('sentiment', MySentimentIntensityAnalyzer(), 'Text')\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feature_extraction', feature_extractor), \n",
    "    ('classifier', KNeighborsClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_extractor\u001b[39m.\u001b[39;49mfit_transform(sotu)\n",
      "File \u001b[0;32m~/source/text-classification/.env/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/source/text-classification/.env/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:738\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# determine if concatenated output will be sparse or not\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(sparse\u001b[39m.\u001b[39missparse(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m Xs):\n\u001b[0;32m--> 738\u001b[0m     nnz \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(X\u001b[39m.\u001b[39;49mnnz \u001b[39mif\u001b[39;49;00m sparse\u001b[39m.\u001b[39;49missparse(X) \u001b[39melse\u001b[39;49;00m X\u001b[39m.\u001b[39;49msize \u001b[39mfor\u001b[39;49;00m X \u001b[39min\u001b[39;49;00m Xs)\n\u001b[1;32m    739\u001b[0m     total \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m    740\u001b[0m         X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X) \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39msize \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m Xs\n\u001b[1;32m    741\u001b[0m     )\n\u001b[1;32m    742\u001b[0m     density \u001b[39m=\u001b[39m nnz \u001b[39m/\u001b[39m total\n",
      "File \u001b[0;32m~/source/text-classification/.env/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:738\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# determine if concatenated output will be sparse or not\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(sparse\u001b[39m.\u001b[39missparse(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m Xs):\n\u001b[0;32m--> 738\u001b[0m     nnz \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(X\u001b[39m.\u001b[39mnnz \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X) \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39;49msize \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m Xs)\n\u001b[1;32m    739\u001b[0m     total \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m    740\u001b[0m         X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X) \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39msize \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m Xs\n\u001b[1;32m    741\u001b[0m     )\n\u001b[1;32m    742\u001b[0m     density \u001b[39m=\u001b[39m nnz \u001b[39m/\u001b[39m total\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "feature_extractor.fit_transform(sotu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Holdout validate pipeline \"\"\"\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "subset = sotu.query(\"Party == 'Democratic' | Party == 'Republican'\")\n",
    "\n",
    "splits = GroupShuffleSplit(1, test_size=0.1).split(subset, groups=subset['Name'])\n",
    "for train_idx, test_idx in splits:\n",
    "    sotu_train = subset.iloc[train_idx]\n",
    "    sotu_test = subset.iloc[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "cv_splits = GroupShuffleSplit(n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9931.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.70s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.71s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.71s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.73s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.74s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.75s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.75s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.76s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9931.77s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.525 total time=   3.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.606 total time=   4.3s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.722 total time=   3.9s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.568 total time=   4.3s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.282 total time=   4.3s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.625 total time=   4.1s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.545 total time=   4.2s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.541 total time=   4.0s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.750 total time=   4.0s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.359 total time=   4.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.525 total time=   3.9s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.486 total time=   3.7s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.576 total time=   4.0s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.694 total time=   3.9s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.333 total time=   4.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.550 total time=   4.2s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.545 total time=   4.3s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.514 total time=   4.5s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.556 total time=   4.3s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.359 total time=   4.6s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.500 total time=   5.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.576 total time=   6.0s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.595 total time=   5.8s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.694 total time=   5.5s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.308 total time=   6.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.600 total time=   5.7s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.545 total time=   6.2s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.541 total time=   6.1s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.359 total time=   6.0s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.750 total time=   5.8s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.500 total time=   5.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.606 total time=   6.1s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.541 total time=   6.2s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.694 total time=   5.9s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.575 total time=   5.8s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.359 total time=   6.5s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.514 total time=   5.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.576 total time=   6.3s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.556 total time=   5.7s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.385 total time=   6.3s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.500 total time=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/f006pfk/source/text-classification/.env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.576 total time=   8.3s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.595 total time=   8.0s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.600 total time=   7.3s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.722 total time=   7.7s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.282 total time=   8.3s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.568 total time=   7.2s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.750 total time=   6.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.545 total time=   8.1s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.359 total time=   7.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9967.41s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9968.39s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9969.06s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.500 total time=   6.0s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.606 total time=   6.2s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.541 total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9969.96s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9970.61s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.359 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9971.42s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.694 total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9975.15s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9975.83s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.575 total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9976.52s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.576 total time=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9977.50s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.514 total time=   6.4s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.525 total time=   3.6s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.385 total time=   7.4s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.556 total time=   6.7s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.568 total time=   4.0s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.606 total time=   4.3s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.722 total time=   4.1s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.282 total time=   4.2s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.545 total time=   4.3s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.625 total time=   4.2s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.359 total time=   4.4s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.541 total time=   4.3s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.750 total time=   4.1s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.525 total time=   4.1s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.576 total time=   4.4s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.486 total time=   4.2s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.333 total time=   4.4s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.694 total time=   4.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.550 total time=   4.4s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.545 total time=   4.4s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.514 total time=   4.3s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.556 total time=   4.3s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.359 total time=   4.8s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.500 total time=   5.6s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.576 total time=   5.9s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.595 total time=   6.0s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.722 total time=   5.3s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.308 total time=   6.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.600 total time=   5.7s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.545 total time=   6.3s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.541 total time=   6.1s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.750 total time=   5.7s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.359 total time=   6.3s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.500 total time=   5.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.606 total time=   6.3s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.541 total time=   6.0s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.694 total time=   5.7s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.359 total time=   6.3s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.575 total time=   6.0s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.576 total time=   6.3s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.514 total time=   6.2s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.556 total time=   5.9s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.385 total time=   6.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.500 total time=   7.9s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.576 total time=   8.2s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.595 total time=   8.0s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.722 total time=   7.5s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.282 total time=   8.4s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.600 total time=   7.7s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.545 total time=   8.1s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.568 total time=   7.8s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.750 total time=   7.4s\n",
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.359 total time=   8.5s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.500 total time=   7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10012.39s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.541 total time=   7.4s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.606 total time=   7.8s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.694 total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10014.31s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.359 total time=   8.2s\n",
      "[CV 1/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.575 total time=   8.2s\n",
      "[CV 2/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.576 total time=   8.5s\n",
      "[CV 3/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.514 total time=   8.3s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.475 total time=   5.3s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.576 total time=   5.7s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.622 total time=   5.7s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.385 total time=   6.5s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.722 total time=   7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10023.61s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.385 total time=  12.7s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.475 total time=   8.3s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.636 total time=   8.6s\n",
      "[CV 5/5] END classifier__n_neighbors=1, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.556 total time=  13.1s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.405 total time=   8.3s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.385 total time=   8.0s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.667 total time=   7.5s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.375 total time=   6.8s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.636 total time=   6.0s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.556 total time=   4.8s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.385 total time=   5.5s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.486 total time=   5.2s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.450 total time=   5.1s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.545 total time=   5.0s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.486 total time=   5.0s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.389 total time=   4.8s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.487 total time=   5.4s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.475 total time=   6.4s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.576 total time=   6.8s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.568 total time=   6.6s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.722 total time=   6.4s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.385 total time=   7.0s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.450 total time=   6.7s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.351 total time=   6.7s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.636 total time=   7.0s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.667 total time=   6.6s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.333 total time=   7.5s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.400 total time=   7.1s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.636 total time=   7.4s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.459 total time=   7.3s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.528 total time=   6.8s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.385 total time=   7.3s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.425 total time=   6.9s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.485 total time=   7.3s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.486 total time=   7.3s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.417 total time=   7.3s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.436 total time=   7.9s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.450 total time=   9.5s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.694 total time=   8.4s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.576 total time=   9.5s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.568 total time=   9.4s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.385 total time=  10.2s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.450 total time=   9.7s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.636 total time=   9.7s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.351 total time=   9.5s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.667 total time=   8.6s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.333 total time=   9.9s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.400 total time=   9.4s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.459 total time=   9.7s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.636 total time=  10.1s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.528 total time=   8.7s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.385 total time=  10.1s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.425 total time=   9.2s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.475 total time=   4.7s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.486 total time=   9.4s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.485 total time=   9.9s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.436 total time=   9.7s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=uniform, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.417 total time=   9.0s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.576 total time=   4.5s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.622 total time=   4.5s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.385 total time=   4.6s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=10;, score=0.694 total time=   4.1s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.475 total time=   4.2s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.636 total time=   4.2s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.405 total time=   3.9s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.385 total time=   4.2s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=15;, score=0.667 total time=   4.0s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.375 total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10073.21s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.636 total time=   4.1s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.486 total time=   4.0s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.556 total time=   3.7s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=20;, score=0.385 total time=   4.1s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.450 total time=   4.2s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.545 total time=   4.6s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.486 total time=   4.4s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.487 total time=   4.5s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 1), feature_extraction__topics__num_topics=25;, score=0.389 total time=   4.4s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.475 total time=   5.9s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.576 total time=   6.3s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.568 total time=   6.1s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.694 total time=   6.0s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.450 total time=   6.1s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=10;, score=0.385 total time=   6.4s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.636 total time=   6.3s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.351 total time=   6.3s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.333 total time=   6.6s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=15;, score=0.667 total time=   6.1s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.400 total time=   6.1s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.459 total time=   6.1s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.636 total time=   6.5s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.385 total time=   6.6s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=20;, score=0.528 total time=   6.0s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.425 total time=   5.7s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.485 total time=   6.0s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.486 total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10089.30s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.436 total time=   5.7s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 2), feature_extraction__topics__num_topics=25;, score=0.417 total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10091.69s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.450 total time=   7.1s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.576 total time=   7.6s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.568 total time=   7.3s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.694 total time=   6.9s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=10;, score=0.385 total time=   7.9s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.450 total time=   7.5s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.636 total time=   8.0s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.351 total time=   7.9s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.333 total time=   8.3s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=15;, score=0.667 total time=   7.4s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.400 total time=   7.6s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.459 total time=   7.7s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.528 total time=   7.4s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.636 total time=   8.4s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=20;, score=0.385 total time=   8.0s\n",
      "[CV 1/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.425 total time=   7.4s\n",
      "[CV 2/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.485 total time=   7.4s\n",
      "[CV 3/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.486 total time=   6.7s\n",
      "[CV 5/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.417 total time=   6.1s\n",
      "[CV 4/5] END classifier__n_neighbors=3, classifier__weights=distance, feature_extraction__tfidf__ngram_range=(1, 3), feature_extraction__topics__num_topics=25;, score=0.436 total time=   7.0s\n",
      "Best parameter (CV score=0.564):\n",
      "{'classifier__n_neighbors': 1, 'classifier__weights': 'uniform', 'feature_extraction__tfidf__ngram_range': (1, 3), 'feature_extraction__topics__num_topics': 15}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Grid search hyperparamters \"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'feature_extraction__tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'classifier__n_neighbors': [1, 3],     \n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "    # 'classifier__criterion': ['gini', 'entropy'],\n",
    "    # 'classifier__max_depth': [3, 5, 10, 20],\n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=cv_splits, verbose=3)\n",
    "search.fit(sotu_train, sotu_train['Party'], groups=sotu_train['Name'])\n",
    "print(f\"Best parameter (CV score={search.best_score_:0.3f}):\")\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Democratic       0.20      0.25      0.22         4\n",
      "  Republican       0.80      0.75      0.77        16\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.50      0.50      0.50        20\n",
      "weighted avg       0.68      0.65      0.66        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(sotu_test['Party'], search.predict(sotu_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech to classify:\n",
      "year=1889, president=Benjamin Harrison, party=Republican\n",
      "Most similar speech:\n",
      "year=1885, president=Grover Cleveland, party=Democratic\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1890, president=Benjamin Harrison, party=Republican\n",
      "Most similar speech:\n",
      "year=1879, president=Rutherford B. Hayes, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1891, president=Benjamin Harrison, party=Republican\n",
      "Most similar speech:\n",
      "year=1885, president=Grover Cleveland, party=Democratic\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1892, president=Benjamin Harrison, party=Republican\n",
      "Most similar speech:\n",
      "year=1894, president=Grover Cleveland, party=Democratic\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1901, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1902, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1903, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1899, president=William McKinley, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1904, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1905, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1906, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1907, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1908, president=Theodore Roosevelt, party=Republican\n",
      "Most similar speech:\n",
      "year=1912, president=William Howard Taft, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1978, president=Jimmy Carter, party=Democratic\n",
      "Most similar speech:\n",
      "year=1983, president=Ronald Reagan, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1979, president=Jimmy Carter, party=Democratic\n",
      "Most similar speech:\n",
      "year=1983, president=Ronald Reagan, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1980, president=Jimmy Carter, party=Democratic\n",
      "Most similar speech:\n",
      "year=1953, president=Harry S. Truman, party=Democratic\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1981, president=Jimmy Carter, party=Democratic\n",
      "Most similar speech:\n",
      "year=1961, president=Dwight D. Eisenhower, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1989, president=George H. W. Bush, party=Republican\n",
      "Most similar speech:\n",
      "year=1984, president=Ronald Reagan, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1990, president=George H. W. Bush, party=Republican\n",
      "Most similar speech:\n",
      "year=1988, president=Ronald Reagan, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1991, president=George H. W. Bush, party=Republican\n",
      "Most similar speech:\n",
      "year=1986, president=Ronald Reagan, party=Republican\n",
      "----------\n",
      "Speech to classify:\n",
      "year=1992, president=George H. W. Bush, party=Republican\n",
      "Most similar speech:\n",
      "year=2009, president=Barack Obama, party=Democratic\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "distance, neighbor = search.best_estimator_[1].kneighbors(search.best_estimator_[0].transform(sotu_test))\n",
    "for idx, speech in sotu_test.reset_index().iterrows():\n",
    "    print('Speech to classify:')\n",
    "    print(f\"year={speech['Year']}, president={speech['Name']}, party={speech['Party']}\")\n",
    "    print('Most similar speech:')\n",
    "    print(f\"year={sotu_train.iloc[neighbor[idx][0], 0]}, president={sotu_train.iloc[neighbor[idx][0], 1]}, party={sotu_train.iloc[neighbor[idx][0], 4]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb1c3ed5e4eeda39f08ae68d3f6b00a1be81574cc2015927ebcc23db4af570f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
